<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3.1"/>
<title>OpenANN: OpenANN::MBSGD Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="openann-logo-small.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenANN
   &#160;<span id="projectnumber">1.1.0</span>
   </div>
   <div id="projectbrief">An open source library for artificial neural networks.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>List</span></a></li>
      <li><a href="classes.html"><span>Index</span></a></li>
      <li><a href="inherits.html"><span>Inheritance</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Friends</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(11)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceOpenANN.html">OpenANN</a></li><li class="navelem"><a class="el" href="classOpenANN_1_1MBSGD.html">MBSGD</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="classOpenANN_1_1MBSGD-members.html">List of all members</a> &#124;
<a href="#pub-methods">Public Member Functions</a>  </div>
  <div class="headertitle">
<div class="title">OpenANN::MBSGD Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Mini-batch stochastic gradient descent.  
 <a href="classOpenANN_1_1MBSGD.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="MBSGD_8h_source.html">MBSGD.h</a>&gt;</code></p>
<div id="dynsection-0" onclick="return toggleVisibility(this)" class="dynheader closed" style="cursor:pointer;">
  <img id="dynsection-0-trigger" src="closed.png" alt="+"/> Inheritance diagram for OpenANN::MBSGD:</div>
<div id="dynsection-0-summary" class="dynsummary" style="display:block;">
</div>
<div id="dynsection-0-content" class="dyncontent" style="display:none;">
<div class="center"><img src="classOpenANN_1_1MBSGD__inherit__graph.png" border="0" usemap="#OpenANN_1_1MBSGD_inherit__map" alt="Inheritance graph"/></div>
<map name="OpenANN_1_1MBSGD_inherit__map" id="OpenANN_1_1MBSGD_inherit__map">
<area shape="rect" id="node2" href="classOpenANN_1_1Optimizer.html" title="The common interface of all optimization algorithms." alt="" coords="5,5,152,195"/></map>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:af5f366446afcd555ad69c2f49a8ae979"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#af5f366446afcd555ad69c2f49a8ae979">MBSGD</a> (double learningRate=0.01, double momentum=0.5, int batchSize=10, bool nesterov=false, double learningRateDecay=1.0, double minimalLearningRate=0.0, double momentumGain=0.0, double maximalMomentum=1.0, double minGain=1.0, double maxGain=1.0)</td></tr>
<tr class="memdesc:af5f366446afcd555ad69c2f49a8ae979"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create mini-batch stochastic gradient descent optimizer.  <a href="#af5f366446afcd555ad69c2f49a8ae979">More...</a><br/></td></tr>
<tr class="separator:af5f366446afcd555ad69c2f49a8ae979"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0e4135fb34527af86d866dffb831872"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#aa0e4135fb34527af86d866dffb831872">~MBSGD</a> ()</td></tr>
<tr class="separator:aa0e4135fb34527af86d866dffb831872"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a0da36dbbd0b724b064c68337454c8b"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#a2a0da36dbbd0b724b064c68337454c8b">setOptimizable</a> (<a class="el" href="classOpenANN_1_1Optimizable.html">Optimizable</a> &amp;opt)</td></tr>
<tr class="memdesc:a2a0da36dbbd0b724b064c68337454c8b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pass the objective function.  <a href="#a2a0da36dbbd0b724b064c68337454c8b">More...</a><br/></td></tr>
<tr class="separator:a2a0da36dbbd0b724b064c68337454c8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abd7daf44e185e58271f008e1daecb6bd"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#abd7daf44e185e58271f008e1daecb6bd">setStopCriteria</a> (const <a class="el" href="classOpenANN_1_1StoppingCriteria.html">StoppingCriteria</a> &amp;stop)</td></tr>
<tr class="memdesc:abd7daf44e185e58271f008e1daecb6bd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pass the stop criteria.  <a href="#abd7daf44e185e58271f008e1daecb6bd">More...</a><br/></td></tr>
<tr class="separator:abd7daf44e185e58271f008e1daecb6bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa605599beec817abd81bdbfd38ab06bd"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#aa605599beec817abd81bdbfd38ab06bd">optimize</a> ()</td></tr>
<tr class="memdesc:aa605599beec817abd81bdbfd38ab06bd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Optimize until the optimization meets the stop criteria.  <a href="#aa605599beec817abd81bdbfd38ab06bd">More...</a><br/></td></tr>
<tr class="separator:aa605599beec817abd81bdbfd38ab06bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a78274c3bf331e37213512282aea3878c"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#a78274c3bf331e37213512282aea3878c">step</a> ()</td></tr>
<tr class="memdesc:a78274c3bf331e37213512282aea3878c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Execute one optimization step.  <a href="#a78274c3bf331e37213512282aea3878c">More...</a><br/></td></tr>
<tr class="separator:a78274c3bf331e37213512282aea3878c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4a9fc08de6e6d1a7d73782fc1bf10b4"><td class="memItemLeft" align="right" valign="top">virtual Eigen::VectorXd&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#af4a9fc08de6e6d1a7d73782fc1bf10b4">result</a> ()</td></tr>
<tr class="memdesc:af4a9fc08de6e6d1a7d73782fc1bf10b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Determine the best result.  <a href="#af4a9fc08de6e6d1a7d73782fc1bf10b4">More...</a><br/></td></tr>
<tr class="separator:af4a9fc08de6e6d1a7d73782fc1bf10b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af986194f6d3719467f9dd43b120afbd8"><td class="memItemLeft" align="right" valign="top">virtual std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1MBSGD.html#af986194f6d3719467f9dd43b120afbd8">name</a> ()</td></tr>
<tr class="memdesc:af986194f6d3719467f9dd43b120afbd8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the name of the optimization algorithm.  <a href="#af986194f6d3719467f9dd43b120afbd8">More...</a><br/></td></tr>
<tr class="separator:af986194f6d3719467f9dd43b120afbd8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classOpenANN_1_1Optimizer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classOpenANN_1_1Optimizer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classOpenANN_1_1Optimizer.html">OpenANN::Optimizer</a></td></tr>
<tr class="memitem:a5c276588a67cb08e5efd2be9bf6b30d1 inherit pub_methods_classOpenANN_1_1Optimizer"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOpenANN_1_1Optimizer.html#a5c276588a67cb08e5efd2be9bf6b30d1">~Optimizer</a> ()</td></tr>
<tr class="separator:a5c276588a67cb08e5efd2be9bf6b30d1 inherit pub_methods_classOpenANN_1_1Optimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Mini-batch stochastic gradient descent. </p>
<p>This implementation of gradient descent has some modifications:</p>
<ul>
<li>it is stochastic, we update the weights with a randomly chosen subset of the training set to escape local minima and thus increase the generalization capabilities</li>
<li>we use a momentum to smooth the search direction</li>
<li>each weight has an adaptive learning rate</li>
<li>we can decrease the learning rate during optimization</li>
<li>we can increase the momentum during optimization</li>
</ul>
<p>A good introduction to optimization with <a class="el" href="classOpenANN_1_1MBSGD.html" title="Mini-batch stochastic gradient descent.">MBSGD</a> can be found in Geoff Hinton's Coursera course "Neural Networks for Machine Learning". A detailed description of this implementation follows.</p>
<p>When the batch size equals 1, the algorithms degenerates to stochastic gradient descent. When it equals the training set size, the algorithm is like batch gradient descent.</p>
<p>Standard mini-batch stochastic gradient descent updates the weight vector w in step t through</p>
<p><img class="formulaInl" alt="$ w^t = w^{t-1} - \frac{\alpha}{|B_t|} \sum_{n \in B_t} \nabla E_n(w) $" src="form_46.png"/></p>
<p>or</p>
<p><img class="formulaInl" alt="$ \Delta w^t = - \frac{\alpha}{|B_t|} \sum_{n \in B_t} \nabla E_n(w), \quad w^t = w^{t-1} + \Delta w^t, $" src="form_47.png"/></p>
<p>where <img class="formulaInl" alt="$ \alpha $" src="form_26.png"/> is the learning rate and <img class="formulaInl" alt="$ B_t $" src="form_48.png"/> is the set of indices of the t-th mini-batch, which is drawn randomly. The random order of gradients prevents us from getting stuck in local minima. This is an advantage over batch gradient descent. However, we must not make the batch size too small. A bigger batch size makes the optimization more robust against noise of the training set. A reasonable batch size is between 10 and 100. The learning rate has to be within [0, 1). High learning rates can result in divergence, i.e. the error increases. Too low learning rates might make learning too slow, i.e. the number of epochs required to find an optimum might be infeasibe. A reasonable value for :math:<code>\\alpha</code> is usually within [1e-5, 0.1].</p>
<p>A momentum can increase the optimization stability. In this case, the update rule is</p>
<p><img class="formulaInl" alt="$ \Delta w^t = \eta \Delta w^{t-1} - \frac{\alpha}{|B_t|} \sum_{n \in B_t} \nabla E_n(w), \quad w^t = w^{t-1} + \Delta w^t, $" src="form_49.png"/></p>
<p>where <img class="formulaInl" alt="$ \eta $" src="form_50.png"/> is called momentum and must lie in [0, 1). The momentum term incorporates past gradients with exponentially decaying influence. This reduces changes of the search direction. An intuitive explanation of this update rule is: we regard w as the position of a ball that is rolling down a hill. The gradient represents its acceleration and the acceleration modifies its momentum.</p>
<p>Another type of momentum that is available in this implementation is the Nesterov's accelerated gradient (NAG). For smooth convex functions, NAG achieves a convergence rate of <img class="formulaInl" alt="$ O(\frac{1}{T^2}) $" src="form_51.png"/> instead of <img class="formulaInl" alt="$ O(\frac{1}{T}) $" src="form_52.png"/> [1]. The update rule only differs in where we calculate the gradient.</p>
<p><img class="formulaInl" alt="$ \Delta w^t = \eta \Delta w^{t-1} - \frac{\alpha}{|B_t|} \sum_{n \in B_t} \nabla E_n(w + \eta \Delta w^{t-1}), \quad w^t = w^{t-1} + \Delta w^t, $" src="form_53.png"/></p>
<p>Another trick is using different learning rates for each weight. For each weight <img class="formulaInl" alt="$ w_{ji} $" src="form_54.png"/> we can introduce a gain <img class="formulaInl" alt="$ g_{ji} $" src="form_55.png"/> which will be multiplied with the learning rate so that we obtain an update rule for each weight</p>
<p><img class="formulaInl" alt="$ \Delta w_{ji}^t = \eta \Delta w_{ji}^{t-1} - \frac{\alpha g_{ji}^{t-1}}{|B_t|} \sum_{n \in B_t} \nabla E_n(w_{ji}), \quad w_{ji}^t = w_{ji}^{t-1} + \Delta w_{ji}^t, $" src="form_56.png"/></p>
<p>where <img class="formulaInl" alt="$ g_{ji}^0 = 1 $" src="form_57.png"/> and <img class="formulaInl" alt="$ g_{ji} $" src="form_55.png"/> will be increased by 0.05 if <img class="formulaInl" alt="$ \Delta w_{ji}^t \Delta w_{ji}^{t-1} \geq 0 $" src="form_58.png"/>, i.e. the sign of the search direction did not change and <img class="formulaInl" alt="$ g_{ji} $" src="form_55.png"/> will be multiplied by 0.95 otherwise. We set a minimum and a maximum value for each gain. Usually these are 0.1 and 10 or 0.001 and 100 respectively.</p>
<p>During optimization it often makes sense to start with a more global search, i.e. with a high learning rate and decrease the learning rate as we approach the minimum so that we obtain an update rule for the learning rate:</p>
<p><img class="formulaInl" alt="$ \alpha^t = max(\alpha_{decay} \alpha^{t-1}, \alpha_{min}). $" src="form_59.png"/></p>
<p>In addition, we can allow the optimizer to change the search direction more often at the beginning of the optimization and reduce this possibility at the end. To do this, we can start with a low momentum and increase it over time until we reach a maximum:</p>
<p><img class="formulaInl" alt="$ \eta^t = min(\eta^{t-1} + \eta_{inc}, \eta_{max}). $" src="form_60.png"/></p>
<p>[1] Sutskever, Ilya; Martens, James; Dahl, George; Hinton, Geoffrey: On the importance of initialization and momentum in deep learning, International Conference on Machine Learning, 2013. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="af5f366446afcd555ad69c2f49a8ae979"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">OpenANN::MBSGD::MBSGD </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em> = <code>0.01</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em> = <code>0.5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batchSize</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>nesterov</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRateDecay</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>minimalLearningRate</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentumGain</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>maximalMomentum</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>minGain</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>maxGain</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Create mini-batch stochastic gradient descent optimizer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">learningRate</td><td>learning rate (usually called alpha); range: (0, 1] </td></tr>
    <tr><td class="paramname">momentum</td><td>momentum coefficient (usually called eta); range: [0, 1) </td></tr>
    <tr><td class="paramname">batchSize</td><td>size of the mini-batches; range: [1, N], where N is the size of the training set </td></tr>
    <tr><td class="paramname">nesterov</td><td>use nesterov's accelerated momentum </td></tr>
    <tr><td class="paramname">learningRateDecay</td><td>will be multiplied with the learning rate after each weight update; range: (0, 1] </td></tr>
    <tr><td class="paramname">minimalLearningRate</td><td>minimum value for the learning rate; range: [0, 1] </td></tr>
    <tr><td class="paramname">momentumGain</td><td>will be added to the momentum after each weight update; range: [0, 1) </td></tr>
    <tr><td class="paramname">maximalMomentum</td><td>maximum value for the momentum; range [0, 1] </td></tr>
    <tr><td class="paramname">minGain</td><td>minimum factor for individual learning rates </td></tr>
    <tr><td class="paramname">maxGain</td><td>maximum factor for individual learning rates </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aa0e4135fb34527af86d866dffb831872"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">OpenANN::MBSGD::~MBSGD </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="af986194f6d3719467f9dd43b120afbd8"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual std::string OpenANN::MBSGD::name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the name of the optimization algorithm. </p>
<dl class="section return"><dt>Returns</dt><dd>name of the optimization algorithm </dd></dl>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#ae622b25a58b293c9e84b255289bf0d7d">OpenANN::Optimizer</a>.</p>

</div>
</div>
<a class="anchor" id="aa605599beec817abd81bdbfd38ab06bd"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void OpenANN::MBSGD::optimize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Optimize until the optimization meets the stop criteria. </p>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#a82dd1d0bb3174099076c5df98114c7bc">OpenANN::Optimizer</a>.</p>

</div>
</div>
<a class="anchor" id="af4a9fc08de6e6d1a7d73782fc1bf10b4"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual Eigen::VectorXd OpenANN::MBSGD::result </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Determine the best result. </p>
<dl class="section return"><dt>Returns</dt><dd>the best parameter the algorithm found </dd></dl>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#aa1fe7b1a0ee5075465dd2fcab268d187">OpenANN::Optimizer</a>.</p>

</div>
</div>
<a class="anchor" id="a2a0da36dbbd0b724b064c68337454c8b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void OpenANN::MBSGD::setOptimizable </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classOpenANN_1_1Optimizable.html">Optimizable</a> &amp;&#160;</td>
          <td class="paramname"><em>optimizable</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Pass the objective function. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">optimizable</td><td>objective function, e. g. error function of an ANN </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#a6eb04ebef8105a1c7507cfd785b96a3c">OpenANN::Optimizer</a>.</p>

</div>
</div>
<a class="anchor" id="abd7daf44e185e58271f008e1daecb6bd"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void OpenANN::MBSGD::setStopCriteria </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classOpenANN_1_1StoppingCriteria.html">StoppingCriteria</a> &amp;&#160;</td>
          <td class="paramname"><em>sc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Pass the stop criteria. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">sc</td><td>the parameters used to stop the optimization </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#a62ca350d18cf4cd88f6e600ea843f6a5">OpenANN::Optimizer</a>.</p>

</div>
</div>
<a class="anchor" id="a78274c3bf331e37213512282aea3878c"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool OpenANN::MBSGD::step </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Execute one optimization step. </p>

<p>Implements <a class="el" href="classOpenANN_1_1Optimizer.html#a843fc2d70cfd87dbd0212192324226cf">OpenANN::Optimizer</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>OpenANN/optimization/<a class="el" href="MBSGD_8h_source.html">MBSGD.h</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Sep 26 2013 23:29:29 for OpenANN by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.3.1
</small></address>
</body>
</html>
